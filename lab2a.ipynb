{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2a.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMkFZ6Typd7wi6rkzvikU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benwtks/machine-learning/blob/master/lab2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CBds3_RLeW_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar4bs8nvKNeZ"
      },
      "source": [
        "# Linear function for score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYtNg2dxBIdf"
      },
      "source": [
        "The score for class $k$ at data point $x$ is given by $$a_k = w_{k0} + \\sum_j w_{kj}x_j=w_0+\\mathbf{w}_k^\\top\\mathbf{x}$$\n",
        "\n",
        "The predicted probability is $$\\hat{y}_k = \\exp(a_k)/\\sum_i\\exp(a_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-j_NjxXCVx_"
      },
      "source": [
        "## For loop implementation of probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDdO07dMICKP"
      },
      "source": [
        "def softmax_prob_forloop(W, b, inputs): # output is datalen-by-C (NumPy, no JAX here)\n",
        "    # W is (C-by-dim) of weights\n",
        "    # b is C-dimensional vector of biases\n",
        "    # inputs is dim-by-datalen\n",
        "    dim, datalen = np.shape(inputs) # how many dimensions, points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    score = np.zeros((c, datalen))\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = b[ci]\n",
        "            for dk in range(dim):\n",
        "                score[ci, lj] += W[ci, dk]*inputs[dk, lj]\n",
        "    maxes = np.zeros(datalen)\n",
        "    for lj in range(datalen):\n",
        "        maxes[lj] = np.max(score[:, lj])\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = score[ci, lj] - maxes[lj]\n",
        "    # subtract off the largest score from the bias of each class \n",
        "    # This is for stability to underflow/overflow when exponentiating\n",
        "    expscore = np.exp(score)\n",
        "    norm_factor = np.diag(1/np.sum(expscore, axis=0))\n",
        "    return np.dot(expscore, norm_factor).T  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M2kwW1UMKoe",
        "outputId": "578bfa1f-2842-4b34-d1e2-21e87e20a1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "W = np.linspace(1,5,20)\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.         1.21052632 1.42105263 1.63157895 1.84210526 2.05263158\n",
            " 2.26315789 2.47368421 2.68421053 2.89473684 3.10526316 3.31578947\n",
            " 3.52631579 3.73684211 3.94736842 4.15789474 4.36842105 4.57894737\n",
            " 4.78947368 5.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFnrZZHlCaC3"
      },
      "source": [
        "## Vector implementation of probability (w/ JAX)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_OzsI1JCfhs"
      },
      "source": [
        "def softmax_prob1(W, b, inputs):  # output is datalen-by-C\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    linear_part = jnp.dot(W, inputs) # (C-by-dim)*(dim-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(linear_part, axis=0) # largest of the class scores for each data point\n",
        "    bias_offset = jnp.dot(jnp.diag(b),jnp.ones((c, datalen))) # (C-by-C)*(C-by-L)\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(linear_part + bias_offset - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh7O5nbQMVIE"
      },
      "source": [
        "In what follows, the trick of setting the zeroth feature to be 1 is used to absorb the constant  $w0$  into the dot product. Redefine the input data to be $$x=(x1,…,xp)⟶x=(1,x1,…,xp).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A03kaf_TMuqR"
      },
      "source": [
        "Correspondingly redefining the weight vectors to be $\\mathbf{w}=(w_0, w_1, \\ldots, w_p)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1thQ740nf6J"
      },
      "source": [
        "def softmax_prob(W, inputs):  \n",
        "    # output is datalen-by-C\n",
        "    # inputs is (dim)-by-datalen\n",
        "    # W is C-by-(dim+1)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(W) # number of classes, C, each class has a bias\n",
        "    inputs = jnp.concatenate((jnp.ones((1,datalen)), inputs), axis=0)\n",
        "    # create inputs (dim+1)-by-datalen \n",
        "    score = jnp.dot(W,inputs) \n",
        "    # (C-by-(1+dim))*((1+dim)-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(score, axis=0) # largest of the class scores for each data point\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(score  - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNhAbAQF0KuM"
      },
      "source": [
        "Wb = jnp.array([[-3., 1.3, 2.0, -1.0], [-6., -2., -3., 1.5], [1., 2.0, 2.0, 2.5], [3., 4.0, 4.0, -2.5]])\n",
        "# Build a toy dataset: 6 3-dim points with C=4  targets dim-by-datalen\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [3.82, -6.11, 3.15],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39],\n",
        "                   [0.14, -0.43, -1.69]]).T # transpose to make it a dim-by-datalen array\n",
        "targets = jnp.array([0, 1, 3, 2, 1, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp9DJLnU0ONu"
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QUYP7kL0QJY"
      },
      "source": [
        "# Automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q38nYQRx0opN"
      },
      "source": [
        "def softmax_xentropy(Wb, inputs, targets, num_classes):\n",
        "    epsilon = 1e-8\n",
        "    ys = get_one_hot(targets, num_classes)\n",
        "    logprobs = -jnp.log(softmax_prob(Wb, inputs)+epsilon)\n",
        "    return jnp.mean(ys*logprobs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPJnw-L0pUM"
      },
      "source": [
        "def get_one_hot(targets, num_classes):\n",
        "    res = jnp.eye(num_classes)[jnp.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[num_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNX8r6OT0rDN"
      },
      "source": [
        "def grad_descent(Wb, inputs, targets, num_classes,  lrate, nsteps):\n",
        "    W1 = Wb\n",
        "    Whist = [W1]\n",
        "    losshist = [softmax_xentropy(W1,inputs, targets, num_classes )]\n",
        "    eta = lrate # learning rate\n",
        "    for i in range(nsteps):        \n",
        "        gWb = grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "        W1 = W1 - eta*gWb\n",
        "        if (i%5 ==0):\n",
        "            Whist.append(W1)\n",
        "            losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))\n",
        "    Whist.append(W1)\n",
        "    losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))    \n",
        "    return W1, Whist, losshist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUbjUWGU0uZ9"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.75, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTBWieiYH239"
      },
      "source": [
        "## Loss history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLPJNgdJH4Ok"
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}